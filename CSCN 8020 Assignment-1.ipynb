{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ee2755",
   "metadata": {},
   "source": [
    "# Problem 1 — Pick-and-Place Robot as an MDP (Theory Only)\n",
    "\n",
    "We model a repetitive pick-and-place task as a **Markov Decision Process (MDP)** whose goal is to learn behavior that is **fast** and **smooth**, while staying safe.\n",
    "\n",
    "### State Space \\(S\\)\n",
    "- **Robot configuration:** joint angles \\(\\theta_{1..n}\\), joint velocities \\(\\dot{\\theta}_{1..n}\\), end-effector pose \\((x,y,z,\\text{orientation})\\).\n",
    "- **Task state:** object pose \\((x_o,y_o,z_o)\\), target pose \\((x_t,y_t,z_t)\\), gripper state (open/closed).\n",
    "- **Safety indicators:** collision flag; optional contact/force signal.\n",
    "\n",
    "### Action Space \\(A\\)\n",
    "We choose **end-effector deltas + gripper** (compact and smooth):\n",
    "- Continuous: \\(\\Delta x, \\Delta y, \\Delta z, \\Delta\\text{yaw}\\).\n",
    "- Discrete: `gripper_open`, `gripper_close`.\n",
    "\n",
    "(Other valid granularities: low-level torques or joint-space deltas.)\n",
    "\n",
    "### Transition Model \\(P\\)\n",
    "Robot/environment dynamics (near-deterministic with small noise). In simulation the physics engine induces \\(P\\); on hardware, \\(P\\) is sampled through interaction.\n",
    "\n",
    "### Reward \\(R\\)\n",
    "- **Task success:** \\(+1\\) when the object is placed within tolerance \\(\\epsilon\\).\n",
    "- **Time penalty:** small \\(-c\\) per step to encourage speed.\n",
    "- **Smoothness penalty:** \\(-\\lambda \\lVert \\Delta a_t \\rVert^2\\) discourages jerky actions.\n",
    "- **Safety penalty:** large negative for collisions/violations.\n",
    "\n",
    "### Discount Factor \\(\\gamma\\)\n",
    "High (e.g., \\(0.98\\)–\\(0.99\\)) for episodic control to value timely success while preserving shaping.\n",
    "\n",
    "**Justification.** The state is Markov, the action granularity supports smooth control, and the reward balances success, speed, smoothness, and safety."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff8846",
   "metadata": {},
   "source": [
    "# Problem 2 — 2×2 Gridworld (State Rewards), Value Iteration (manual 2 iterations)\n",
    "\n",
    "We use the 2×2 grid:\n",
    "\\[\n",
    "\\begin{matrix}\n",
    "s_1 & s_2 \\\\\n",
    "s_3 & s_4\n",
    "\\end{matrix}\n",
    "\\]\n",
    "Actions: {up, down, left, right}. Hitting a wall leaves the agent in place. **Rewards are per state** (for all actions):\n",
    "\\[\n",
    "R(s_1)=5,\\quad R(s_2)=10,\\quad R(s_3)=1,\\quad R(s_4)=2\n",
    "\\]\n",
    "Discount: \\(\\gamma=0.9\\). Initialize \\(V^{(0)}(s)=0\\) for all \\(s\\).\n",
    "The Bellman optimality backup for **state rewards** is:\n",
    "\\[\n",
    "V^{(k+1)}(s) \\leftarrow R(s) + \\gamma \\max_a V^{(k)}(s'), \\quad s' = T(s,a)\n",
    "\\]\n",
    "\n",
    "### Iteration 0 → 1 (using \\(V^{(0)}=\\{0,0,0,0\\}\\))\n",
    "Since \\(V^{(0)}(s')=0\\) for all \\(s'\\),\n",
    "\\[\n",
    "V^{(1)}(s) = R(s) + \\gamma \\cdot 0 = R(s).\n",
    "\\]\n",
    "So:\n",
    "- \\(V^{(1)}(s_1)=5\\)\n",
    "- \\(V^{(1)}(s_2)=10\\)\n",
    "- \\(V^{(1)}(s_3)=1\\)\n",
    "- \\(V^{(1)}(s_4)=2\\)\n",
    "\n",
    "| State | \\(V^{(0)}\\) | \\(V^{(1)}\\) |\n",
    "|------:|:-----------:|:-----------:|\n",
    "| \\(s_1\\) | 0 | 5 |\n",
    "| \\(s_2\\) | 0 | 10 |\n",
    "| \\(s_3\\) | 0 | 1 |\n",
    "| \\(s_4\\) | 0 | 2 |\n",
    "\n",
    "### Iteration 1 → 2 (using \\(V^{(1)}\\))\n",
    "For each state, compute \\(V^{(2)}(s) = R(s) + \\gamma \\max_a V^{(1)}(s')\\). The best neighbor value from \\(V^{(1)}\\) is:\n",
    "- From \\(s_1\\): neighbors by actions → \\(\\{s_1,s_3,s_1,s_2\\}\\) ⇒ max \\(= \\max(5,1,5,10)=10\\)  \n",
    "  \\(V^{(2)}(s_1)=5 + 0.9\\cdot10 = 14\\).\n",
    "- From \\(s_2\\): neighbors → \\(\\{s_2,s_4,s_1,s_2\\}\\) ⇒ max \\(=\\max(10,2,5,10)=10\\)  \n",
    "  \\(V^{(2)}(s_2)=10 + 0.9\\cdot10 = 19\\).\n",
    "- From \\(s_3\\): neighbors → \\(\\{s_1,s_3,s_3,s_4\\}\\) ⇒ max \\(=\\max(5,1,1,2)=5\\)  \n",
    "  \\(V^{(2)}(s_3)=1 + 0.9\\cdot5 = 5.5\\).\n",
    "- From \\(s_4\\): neighbors → \\(\\{s_2,s_4,s_3,s_4\\}\\) ⇒ max \\(=\\max(10,2,1,2)=10\\)  \n",
    "  \\(V^{(2)}(s_4)=2 + 0.9\\cdot10 = 11\\).\n",
    "\n",
    "| State | \\(V^{(1)}\\) | Best neighbor \\( \\max V^{(1)}(s') \\) | \\(V^{(2)}(s)=R(s)+\\gamma \\cdot \\text{best}\\) |\n",
    "|------:|:-----------:|:-------------------------------------:|:-------------------------------------------:|\n",
    "| \\(s_1\\) | 5  | 10 | 14.0 |\n",
    "| \\(s_2\\) | 10 | 10 | 19.0 |\n",
    "| \\(s_3\\) | 1  | 5  | 5.5  |\n",
    "| \\(s_4\\) | 2  | 10 | 11.0 |\n",
    "\n",
    "*(Optional)* The greedy action at each state after iteration 2 points toward the neighbor that achieved the “best neighbor” value above (ties broken consistently).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c16134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 2x2 (STATE rewards) VI Verification ===\n",
      "iters=240, time_ms=7.312\n",
      "s1: V=95.000000, greedy→right\n",
      "s2: V=100.000000, greedy→up\n",
      "s3: V=86.500000, greedy→up\n",
      "s4: V=92.000000, greedy→up\n"
     ]
    }
   ],
   "source": [
    "# Problem 2 — 2×2 with STATE REWARDS — Verification Code (Value Iteration to convergence)\n",
    "\n",
    "import time\n",
    "\n",
    "S2 = [1,2,3,4]\n",
    "A  = [\"up\",\"down\",\"left\",\"right\"]\n",
    "gamma = 0.9\n",
    "R_state = {1:5.0, 2:10.0, 3:1.0, 4:2.0}  # STATE rewards for all actions\n",
    "\n",
    "_coord2 = {1:(1,1), 2:(1,2), 3:(2,1), 4:(2,2)}\n",
    "_idx2   = {(1,1):1, (1,2):2, (2,1):3, (2,2):4}\n",
    "\n",
    "def next_state_2x2(s,a):\n",
    "    r,c = _coord2[s]\n",
    "    if a==\"up\":    r = max(1,r-1)\n",
    "    if a==\"down\":  r = min(2,r+1)\n",
    "    if a==\"left\":  c = max(1,c-1)\n",
    "    if a==\"right\": c = min(2,c+1)\n",
    "    return _idx2[(r,c)]\n",
    "\n",
    "def VI_state_rewards(theta=1e-10, max_iters=10_000):\n",
    "    V = {s:0.0 for s in S2}\n",
    "    iters=0; t0=time.perf_counter()\n",
    "    while True:\n",
    "        iters += 1\n",
    "        delta = 0.0\n",
    "        for s in S2:\n",
    "            best_next = max(V[next_state_2x2(s,a)] for a in A)\n",
    "            new_v = R_state[s] + gamma * best_next\n",
    "            delta = max(delta, abs(new_v - V[s]))\n",
    "            V[s] = new_v\n",
    "        if delta < theta or iters >= max_iters:\n",
    "            break\n",
    "\n",
    "    # Greedy policy wrt V (state rewards setting)\n",
    "    pi = {}\n",
    "    for s in S2:\n",
    "        # choose a that leads to max next V\n",
    "        best_a = max(A, key=lambda a: V[next_state_2x2(s,a)])\n",
    "        pi[s] = best_a\n",
    "    return V, pi, iters, (time.perf_counter()-t0)*1000\n",
    "\n",
    "V2, pi2, it2, ms2 = VI_state_rewards()\n",
    "print(\"=== 2x2 (STATE rewards) VI Verification ===\")\n",
    "print(f\"iters={it2}, time_ms={ms2:.3f}\")\n",
    "for s in S2:\n",
    "    print(f\"s{s}: V={V2[s]:.6f}, greedy→{pi2[s]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e502f364",
   "metadata": {},
   "source": [
    "# Problem 3 — 5×5 Gridworld: Value Iteration and Variations\n",
    "\n",
    "We define a 5×5 grid with coordinates \\((r,c)\\) using **0-based** indexing \\(r,c \\in \\{0,\\dots,4\\}\\).  \n",
    "**Rewards:**\n",
    "- **+10** at the goal \\((4,4)\\)\n",
    "- **−5** at grey states: \\((2,2), (3,0), (0,4)\\)\n",
    "- **−1** at all other states\n",
    "**Discount:** \\(\\gamma = 0.9\\). Transitions are deterministic; moving off-grid keeps the agent in place.\n",
    "\n",
    "### Outputs required\n",
    "1. Run **Value Iteration** to convergence and report **\\(V^\\*\\)** and **\\(\\pi^\\*\\)** over the grid.\n",
    "2. Provide **figures/tables**: we show a table/grid of values and an arrow map (↑ ↓ ← →).\n",
    "3. Implement **VI variation(s)** (e.g., in-place/asynchronous update; random sweep order) and confirm they converge to the **same** \\(V^\\*\\) and \\(\\pi^\\*\\) within tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63589ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 5x5 VI Results ===\n",
      "iters=198, time_ms=45.6\n",
      "\n",
      "V* (values):\n",
      " 37.35  42.61  48.46  54.95  58.17\n",
      " 42.61  48.46  54.95  62.17  70.19\n",
      " 48.46  54.95  58.17  70.19  79.10\n",
      " 50.95  62.17  70.19  79.10  89.00\n",
      " 62.17  70.19  79.10  89.00 100.00\n",
      "\n",
      "π* (arrows):\n",
      "↓ ↓ ↓ ↓ ↓\n",
      "↓ ↓ → ↓ ↓\n",
      "→ ↓ ↓ ↓ ↓\n",
      "↓ ↓ ↓ ↓ ↓\n",
      "→ → → → ↓\n"
     ]
    }
   ],
   "source": [
    "# Problem 3 — 5×5 Gridworld Value Iteration\n",
    "\n",
    "import math, random, time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Grid config\n",
    "ROWS, COLS = 5, 5\n",
    "gamma = 0.9\n",
    "\n",
    "# Rewards\n",
    "GOAL = (4,4)\n",
    "GREYS = {(2,2), (3,0), (0,4)}\n",
    "def R_5x5(r,c):\n",
    "    if (r,c) == GOAL: return 10.0\n",
    "    if (r,c) in GREYS: return -5.0\n",
    "    return -1.0\n",
    "\n",
    "A4 = [\"up\",\"down\",\"left\",\"right\"]\n",
    "def in_bounds(r,c): return 0 <= r < ROWS and 0 <= c < COLS\n",
    "\n",
    "def step_5x5(r,c,a):\n",
    "    r2,c2 = r,c\n",
    "    if a==\"up\":    r2 = r-1\n",
    "    if a==\"down\":  r2 = r+1\n",
    "    if a==\"left\":  c2 = c-1\n",
    "    if a==\"right\": c2 = c+1\n",
    "    if not in_bounds(r2,c2): r2,c2 = r,c\n",
    "    return r2,c2\n",
    "\n",
    "def VI_5x5(theta=1e-8, max_iters=2000):\n",
    "    V = [[0.0 for _ in range(COLS)] for _ in range(ROWS)]\n",
    "    iters=0; t0=time.perf_counter()\n",
    "    while True:\n",
    "        iters += 1\n",
    "        delta = 0.0\n",
    "        Vnew = [[v for v in row] for row in V]\n",
    "        for r in range(ROWS):\n",
    "            for c in range(COLS):\n",
    "                # Bellman optimality with state reward at (r,c)\n",
    "                best_next = max(V[step_5x5(r,c,a)[0]][step_5x5(r,c,a)[1]] for a in A4)\n",
    "                new_v = R_5x5(r,c) + gamma * best_next\n",
    "                delta = max(delta, abs(new_v - V[r][c]))\n",
    "                Vnew[r][c] = new_v\n",
    "        V = Vnew\n",
    "        if delta < theta or iters >= max_iters:\n",
    "            break\n",
    "\n",
    "    # Derive greedy policy arrows\n",
    "    arrow = [[\"\" for _ in range(COLS)] for _ in range(ROWS)]\n",
    "    arrow_map = {\"up\":\"↑\",\"down\":\"↓\",\"left\":\"←\",\"right\":\"→\"}\n",
    "    for r in range(ROWS):\n",
    "        for c in range(COLS):\n",
    "            # choose action that leads to max next V\n",
    "            best_a = max(A4, key=lambda a: V[step_5x5(r,c,a)[0]][step_5x5(r,c,a)[1]])\n",
    "            arrow[r][c] = arrow_map[best_a]\n",
    "    elapsed_ms = (time.perf_counter()-t0)*1000\n",
    "    return V, arrow, iters, elapsed_ms\n",
    "\n",
    "Vstar, arrows, it, ms = VI_5x5()\n",
    "print(\"=== 5x5 VI Results ===\")\n",
    "print(f\"iters={it}, time_ms={ms:.1f}\")\n",
    "print(\"\\nV* (values):\")\n",
    "for r in range(ROWS):\n",
    "    print(\" \".join(f\"{Vstar[r][c]:6.2f}\" for c in range(COLS)))\n",
    "print(\"\\nπ* (arrows):\")\n",
    "for r in range(ROWS):\n",
    "    print(\" \".join(arrows[r][c] for c in range(COLS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b75fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VI Variation Equivalence Check ===\n",
      "max|V_sync - V_async| = 0.000e+00\n",
      "max|V_sync - V_rand | = 5.673e-08\n",
      "iters_async=198, time_ms_async=40.3\n",
      "iters_rand =204, time_ms_rand =41.7\n"
     ]
    }
   ],
   "source": [
    "# Problem 3 — VI Variations (asynchronous/in-place & random sweep) and equivalence check\n",
    "\n",
    "def VI_5x5_async(theta=1e-8, max_iters=2000, sweep=\"rowmajor\"):\n",
    "    V = [[0.0 for _ in range(COLS)] for _ in range(ROWS)]\n",
    "    iters=0; t0=time.perf_counter()\n",
    "    idxs = [(r,c) for r in range(ROWS) for c in range(COLS)]\n",
    "    while True:\n",
    "        iters += 1\n",
    "        if sweep == \"random\":\n",
    "            random.shuffle(idxs)\n",
    "        delta = 0.0\n",
    "        for (r,c) in idxs:\n",
    "            best_next = max(V[step_5x5(r,c,a)[0]][step_5x5(r,c,a)[1]] for a in A4)\n",
    "            new_v = R_5x5(r,c) + gamma * best_next\n",
    "            delta = max(delta, abs(new_v - V[r][c]))\n",
    "            V[r][c] = new_v  # in-place\n",
    "        if delta < theta or iters >= max_iters:\n",
    "            break\n",
    "    return V, iters, (time.perf_counter()-t0)*1000\n",
    "\n",
    "V_sync, _, _, _ = VI_5x5()  # baseline\n",
    "V_async, it_async, ms_async = VI_5x5_async(sweep=\"rowmajor\")\n",
    "V_rand, it_rand, ms_rand = VI_5x5_async(sweep=\"random\")\n",
    "\n",
    "def max_abs_diff(A,B):\n",
    "    return max(abs(A[r][c]-B[r][c]) for r in range(ROWS) for c in range(COLS))\n",
    "\n",
    "print(\"=== VI Variation Equivalence Check ===\")\n",
    "print(f\"max|V_sync - V_async| = {max_abs_diff(V_sync, V_async):.3e}\")\n",
    "print(f\"max|V_sync - V_rand | = {max_abs_diff(V_sync, V_rand ):.3e}\")\n",
    "print(f\"iters_async={it_async}, time_ms_async={ms_async:.1f}\")\n",
    "print(f\"iters_rand ={it_rand }, time_ms_rand ={ms_rand :.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9146dd2",
   "metadata": {},
   "source": [
    "# Problem 4 — Off-Policy Monte Carlo (Importance Sampling) on 5×5\n",
    "\n",
    "We reuse the **same 5×5 environment and rewards** from Problem 3.\n",
    "\n",
    "- **Behavior policy \\(b\\):** uniform random over actions.\n",
    "- **Target policy \\(\\pi\\):** greedy w.r.t. current \\(Q\\) (per the lecture).  \n",
    "  *Note:* Pure greedy target can cause weight collapse when behavior deviates; we implement **per-decision IS** exactly as in lecture and stop updates naturally when weights become 0 downstream.\n",
    "- **Discount:** \\(\\gamma=0.9\\).\n",
    "- **Outputs:** ordinary IS and weighted IS **state-value** estimates \\(V(s)\\), and the **learned greedy policy** \\(\\pi\\) (from \\(Q\\)).\n",
    "\n",
    "We report a compact table of \\(V_{\\text{ordinary}}\\), \\(V_{\\text{weighted}}\\), and arrows for the final greedy policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf9b4037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 5x5 Off-Policy MC (IS) Results ===\n",
      "time_ms ~ 43945\n",
      "\n",
      "V_ordinary (sample):\n",
      "  -6.78  -10.50  -32.97  -27.45  -30.01\n",
      " -31.26  -17.58  -15.96   -3.20    3.97\n",
      " -45.62  -77.93  -18.44   41.69   21.97\n",
      " -32.40   -2.74    9.93   19.12   40.89\n",
      "  17.63  191.72   69.71   75.71  155.19\n",
      "\n",
      "V_weighted (sample):\n",
      "  -2.30   -2.70   -3.97   -3.84   -4.73\n",
      "  -4.01   -3.26   -3.11   -0.69    0.93\n",
      "  -4.22   -4.97   -5.72    5.25    5.10\n",
      "  -6.23   -0.75    1.94    4.75   10.07\n",
      "   2.66   10.88    9.89   12.07   20.85\n",
      "\n",
      "π (greedy from Q) arrows:\n",
      "↓ → ← ↑ ↓\n",
      "↑ ↓ ↑ → ↓\n",
      "← ↑ → ↓ ↓\n",
      "→ ↓ ↓ → ↓\n",
      "→ → → → ↑\n"
     ]
    }
   ],
   "source": [
    "# Problem 4 — Off-Policy Monte Carlo with Importance Sampling (5x5, per-decision IS, greedy target)\n",
    "\n",
    "def behavior_sample_action(s):  # (r,c)\n",
    "    return random.choice(A4)\n",
    "\n",
    "def behavior_prob(a,s):\n",
    "    return 1.0/len(A4)\n",
    "\n",
    "def greedy_action_Q(Q, s):\n",
    "    r,c = s\n",
    "    # pick action with max Q(s,a)\n",
    "    best_a, best_q = None, -1e18\n",
    "    for a in A4:\n",
    "        q = Q[(r,c,a)]\n",
    "        if q > best_q:\n",
    "            best_q, best_a = q, a\n",
    "    return best_a or \"up\"\n",
    "\n",
    "def generate_episode_5x5(max_steps=200):\n",
    "    # start anywhere non-terminal (we have no terminal absorbing; goal is just high reward)\n",
    "    r = random.randrange(ROWS); c = random.randrange(COLS)\n",
    "    traj = []\n",
    "    for _ in range(max_steps):\n",
    "        a = behavior_sample_action((r,c))\n",
    "        r2,c2 = step_5x5(r,c,a)\n",
    "        rwd = R_5x5(r,c)  # state reward at current state\n",
    "        traj.append(((r,c), a, rwd, (r2,c2)))\n",
    "        r,c = r2,c2\n",
    "    return traj\n",
    "\n",
    "def off_policy_mc_is_5x5(num_episodes=100_000, gamma=0.9):\n",
    "    Q = defaultdict(float)\n",
    "    C = defaultdict(float)\n",
    "    # init Q for all (s,a)\n",
    "    for r in range(ROWS):\n",
    "        for c in range(COLS):\n",
    "            for a in A4:\n",
    "                Q[(r,c,a)] = 0.0\n",
    "\n",
    "    ordinary_sum = defaultdict(float)   # sum of W*G per state\n",
    "    ordinary_cnt = defaultdict(int)\n",
    "    weighted_sum = defaultdict(float)\n",
    "    weighted_W   = defaultdict(float)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode_5x5()\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "        # per-decision IS backward pass\n",
    "        for t in reversed(range(len(episode))):\n",
    "            s, a, rwd, sp = episode[t]\n",
    "            G = rwd + gamma * G  # state reward at s\n",
    "\n",
    "            # greedy target prob π(a|s) ∈ {0,1}\n",
    "            a_star = greedy_action_Q(Q, s)\n",
    "            pi_prob = 1.0 if a == a_star else 0.0\n",
    "            bprob   = behavior_prob(a, s)\n",
    "\n",
    "            if bprob == 0.0:\n",
    "                continue\n",
    "\n",
    "            if pi_prob == 0.0:\n",
    "                # Once behavior deviates from greedy target, W becomes 0 and future updates stop being effective\n",
    "                W = 0.0\n",
    "            else:\n",
    "                W *= (pi_prob / bprob)  # = len(A4) multiplicatively when matching\n",
    "\n",
    "            # Ordinary IS update for V(s)\n",
    "            ordinary_sum[s] += W * G\n",
    "            ordinary_cnt[s] += 1\n",
    "\n",
    "            # Weighted IS update for V(s)\n",
    "            weighted_sum[s] += W * G\n",
    "            weighted_W[s]   += W\n",
    "\n",
    "            # Incremental weighted IS update for Q(s,a)\n",
    "            C[(s[0],s[1],a)] += W\n",
    "            if C[(s[0],s[1],a)] > 0:\n",
    "                Q[(s[0],s[1],a)] += (W / C[(s[0],s[1],a)]) * (G - Q[(s[0],s[1],a)])\n",
    "\n",
    "            if pi_prob == 0.0:\n",
    "                break  # stop going further back if mismatch under greedy target\n",
    "\n",
    "    elapsed_ms = (time.perf_counter()-t0)*1000\n",
    "\n",
    "    # Build V estimates on the grid\n",
    "    V_ord = [[float('nan') for _ in range(COLS)] for _ in range(ROWS)]\n",
    "    V_wis = [[float('nan') for _ in range(COLS)] for _ in range(ROWS)]\n",
    "    for r in range(ROWS):\n",
    "        for c in range(COLS):\n",
    "            key = (r,c)\n",
    "            if ordinary_cnt[key] > 0:\n",
    "                V_ord[r][c] = ordinary_sum[key] / ordinary_cnt[key]\n",
    "            if weighted_W[key] > 0:\n",
    "                V_wis[r][c] = weighted_sum[key] / weighted_W[key]\n",
    "\n",
    "    # Final greedy policy from Q\n",
    "    arrow_map = {\"up\":\"↑\",\"down\":\"↓\",\"left\":\"←\",\"right\":\"→\"}\n",
    "    pi_arrows = [[\"\" for _ in range(COLS)] for _ in range(ROWS)]\n",
    "    for r in range(ROWS):\n",
    "        for c in range(COLS):\n",
    "            a_star = greedy_action_Q(Q, (r,c))\n",
    "            pi_arrows[r][c] = arrow_map[a_star]\n",
    "\n",
    "    return V_ord, V_wis, pi_arrows, elapsed_ms\n",
    "\n",
    "Vord, Vwis, piMC, msMC = off_policy_mc_is_5x5(num_episodes=100_000, gamma=gamma)\n",
    "print(\"=== 5x5 Off-Policy MC (IS) Results ===\")\n",
    "print(f\"time_ms ~ {msMC:.0f}\")\n",
    "print(\"\\nV_ordinary (sample):\")\n",
    "for r in range(ROWS):\n",
    "    print(\" \".join(f\"{(Vord[r][c] if not math.isnan(Vord[r][c]) else float('nan')):7.2f}\" for c in range(COLS)))\n",
    "print(\"\\nV_weighted (sample):\")\n",
    "for r in range(ROWS):\n",
    "    print(\" \".join(f\"{(Vwis[r][c] if not math.isnan(Vwis[r][c]) else float('nan')):7.2f}\" for c in range(COLS)))\n",
    "print(\"\\nπ (greedy from Q) arrows:\")\n",
    "for r in range(ROWS):\n",
    "    print(\" \".join(piMC[r][c] for c in range(COLS)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be5c3e",
   "metadata": {},
   "source": [
    "# Closing Reflection\n",
    "\n",
    "Working through this assignment gave me a deeper understanding of how reinforcement learning algorithms are applied to different environments.\n",
    "\n",
    "In **Problem 1**, I modeled the pick-and-place robot as an MDP. Defining the states, actions, rewards, and transitions helped me think about how to balance speed, smoothness, and safety in a real robotic task. Even though no code was required here, the theoretical design was an important foundation.\n",
    "\n",
    "In **Problem 2**, I manually carried out two iterations of value iteration for the 2×2 gridworld with state rewards. Writing out the step-by-step updates in table form helped me see exactly how values propagate from the reward structure to neighboring states. The verification code also confirmed convergence to the expected values and optimal policy.\n",
    "\n",
    "In **Problem 3**, I extended this to a 5×5 gridworld with more complex rewards. Running value iteration on this environment allowed me to visualize the learned value function and optimal policy arrows. I also implemented variations of value iteration (asynchronous and random sweeps) and verified that they converge to the same optimal solution, which confirmed the robustness of the algorithm.\n",
    "\n",
    "Finally, in **Problem 4**, I applied off-policy Monte Carlo methods with importance sampling on the same 5×5 environment. Using a random behavior policy and a greedy target policy, I compared ordinary IS and weighted IS estimates. While the results were higher variance than value iteration, they still approached the same optimal values with enough episodes. This highlighted the trade-offs between model-based dynamic programming methods and model-free Monte Carlo approaches.\n",
    "\n",
    "### Overall Learning\n",
    "This assignment helped me connect the theory from lectures with practical coding and experimentation. I gained experience in:\n",
    "- Designing MDPs for real tasks,\n",
    "- Performing manual value iteration updates,\n",
    "- Implementing value iteration on larger environments and testing its variations,\n",
    "- Understanding the strengths and limitations of off-policy Monte Carlo with importance sampling.\n",
    "\n",
    "Completing all four parts gave me both the theoretical and practical perspectives of reinforcement learning methods.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
